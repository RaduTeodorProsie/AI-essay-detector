# AI detector for essays based on Multinominal Naive Bayes 
This is a tool to classify an essay (or any text, rather) based on multinomial, naive, Bayes. 
Simply put, it analyzes at the word frequencies and decides wether the frequencies are more AI-like or human like.
## How to use
Firstly, clone the repo and `cd` into it
```bash
git clone https://github.com/RaduTeodorProsie/AI-essay-detector.git && cd AI-essay-detector
```
Next, create a new virtual enviroment and activate it
```bash
python -m venv venv
.venv\Scripts\activate # for windows
source venv/bin/activate # for linux and mac
```
Following that, install the necessary dependencies
```bash
pip install -r requirements.txt
```
Now it's time to train the model. You can use your own csv to do so, but I will provide the datasets I used and how to dowload them (please note however, that they should still be processed before the training by using the `cleaner.py` script. If you choose to use the same datasets as I, you can find them here : [training](https://www.kaggle.com/datasets/shanegerami/ai-vs-human-text) and [testing](https://www.kaggle.com/datasets/navjotkaushal/human-vs-ai-generated-essays). You can either download them directly from kaggle or use the API. To find how to run the trainer, run the `trainer.py` script with the `-h` argument. After you ran the trainer (which should take no more than 8 seconds), you should get an output with the result of its training. Here is mine, for example :
```
=== Evaluation on external test set ===
Accuracy: 0.9941818181818182

Classification report:
               precision    recall  f1-score   support

           0     0.9993    0.9891    0.9942      1375
           1     0.9892    0.9993    0.9942      1375

    accuracy                         0.9942      2750
   macro avg     0.9942    0.9942    0.9942      2750
weighted avg     0.9942    0.9942    0.9942      2750


Confusion matrix:
 [[1360   15]
 [   1 1374]]

Model saved to nb_scratch_model.joblib
```
Note however that the result of the training largely depends on the length of the texts used. Generally speaking, the longer a text is, the easier for the classifier to ascertain its class as the differences between the word frequencies of humans and AI are more visible. Empirically speaking, the result isn't as high as the output suggests, and is more around the 85% mark. It especially struggles against texts generated by GPT-5mini and up, but usually classifies correctly texts written by Gemini 2.5 Pro and simmilar models. I have also tested it against essay generated by websites such as [this](https://www.the-good-ai.com/) on deeply human topics such as love, and the classifier still emerged victorious. So it's safe to say that although it has a simple mathemathical model behind it, the large dataset used for its training was worth it. 
